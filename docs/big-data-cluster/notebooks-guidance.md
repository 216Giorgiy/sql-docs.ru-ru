---
title: Как использовать записные книжки в предварительной версии SQL Server 2019 | Документация Майкрософт
description: ''
author: rothja
ms.author: jroth
manager: craigg
ms.date: 10/05/2018
ms.topic: conceptual
ms.prod: sql
ms.openlocfilehash: 137da00959f6f8d3498bb3d063ceb21337266aef
ms.sourcegitcommit: ce4b39bf88c9a423ff240a7e3ac840a532c6fcae
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 10/09/2018
ms.locfileid: "48878017"
---
# <a name="how-to-use-notebooks-in-sql-server-2019-preview"></a>Как использовать записные книжки в предварительной версии SQL Server 2019

В этой статье показано, как запускать записные книжки в кластер SQL Server 2019 больших данных. Также показано, как начать создание собственных записных книжек и как отправлять задания, к кластеру.

## <a name="prerequisites"></a>предварительные требования

Использовать записные книжки, необходимо установить следующие компоненты:

- [Кластер SQL Server 2019 больших данных](deployment-guidance.md)
- [Azure Data Studio](../azure-data-studio/what-is.md)
- [Расширение SQL Server 2019 (Предварительная версия)](../azure-data-studio/sql-server-2019-extension.md).

[!INCLUDE [Limited public preview note](../includes/big-data-cluster-preview-note.md)]

## <a name="connect-to-the-sql-server-big-data-cluster-end-point"></a>Подключение к конечной точке кластера больших данных SQL Server

Вы можете подключиться к различные конечные точки в кластере. Можно подключиться к типу соединения Microsoft SQL Server или в конечной точке кластера SQL Server больших данных.

В студии данных Azure (Предварительная версия), введите **F1** > **новое подключение**и подключитесь к вашей конечной точки SQL Server больших данных кластера.

![изображение 1](media/notebooks-guidance/image1.png)

## <a name="browse-hdfs"></a>Обзор HDFS
После подключения вы сможете просмотреть папку HDFS. WebHDFS запускается после завершения развертывания, и вы сможете для **обновить**, добавьте **новый каталог**, **отправить** файлы, и **удалить**.

![изображение2](media/notebooks-guidance/image2.png)

Эти простые операции позволяют предоставить свои данные в HDFS.

## <a name="launch-new-notebooks"></a>Запустить новые записные книжки

Существует несколько способов, чтобы запустить записную книжку.

1. Из панели мониторинга управления. На создание нового подключения, вы увидите панель мониторинга. Щелкните новую записную книжку задачи с помощью панели мониторинга.

  ![image3](media/notebooks-guidance/image3.png)

1. Щелкните правой кнопкой подключение HDFS/Spark и у вас есть новая записная книжка в контекстном меню

![image4](media/notebooks-guidance/image4.png)

Введите имя записной книжки (пример: *Test.ipynb*) и нажмите кнопку **Сохранить**.

![image5](media/notebooks-guidance/image5.png)

## <a name="supported-kernels-and-attach-to-context"></a>Поддерживаемые версии ядра и присоединить к контексту

В нашей установки записной книжки мы поддерживаем ядрами PySpark и Spark, волшебной команды Spark, которые позволяют писать код Python и Scala, с помощью Spark. Мы также позволяют пользователям выбирать их в целях локальной разработки Python.

![image6](media/notebooks-guidance/image6.png)

При выборе одного из этих ядер, мы установим этот ядра в виртуальной среде и написания кода на поддерживаемых языках.

| Ядра | Описание
|---- |----
|Ядро PySpark| Для написания кода Python, с помощью Spark, вычислений из кластера.
|Ядро Spark|Для написания кода Scala, с помощью Spark, вычислений из кластера.
|Ядро Python|Для написания кода Python для локальной разработки.

Выбор для присоединения предоставляет контекст для ядра для присоединения. При подключении к конечной точке кластера SQL Server больших данных, выбор по умолчанию для присоединения будет этой конечной точке кластера.

![image7](media/notebooks-guidance/image7.png)

> [!NOTE]
> По умолчанию приложения Spark настраивается с помощью драйвера 1 и 3 исполнителей, которые будут выполнены около 8,5 ГБ памяти. Рекомендуемая конфигурация для запуска нескольких сеансов spark — для каждого сервера в кластере, чтобы иметь по крайней мере 32 ГБ памяти (например, в среде AKS использовать **Standard_D8_v3** размеры виртуальных Машин, которые имеют 32 ГБ памяти).

## <a name="hello-world-in-the-different-contexts"></a>Hello world в различных контекстах

### <a name="pyspark-kernel"></a>Ядро Pyspark

Выберите ядро PySpark и в тип ячейки в следующем коде:

![image8](media/notebooks-guidance/image8.png)

Щелкните выполнения и вам следует см. запуск приложения Spark и вы увидите следующие выходные данные:

![image9](media/notebooks-guidance/image9.png)

Результат должен выглядеть примерно так, как на рисунке ниже.

![Image10](media/notebooks-guidance/image10.png)

### <a name="spark-kernel"></a>Ядро Spark
Добавьте новую ячейку кода, нажав кнопку + кода команду панели инструментов.

![Image11](media/notebooks-guidance/image11.png)

Выберите ядро Spark, в раскрывающемся списке для ядра, а также в ячейке введите или вставьте в 

![Image12](media/notebooks-guidance/image12.png)

Нажмите кнопку **запуска** вы увидите запуск приложения Spark и будет создан сеанс Spark **spark** и определяете **HelloWorld** объекта.

Записная книжка должна выглядеть как на следующем рисунке.

![Image13](media/notebooks-guidance/image13.png)

Один раз можно определить объект затем в следующий тип ячейки записной книжки в следующем коде:

![Image14](media/notebooks-guidance/image14.png)

Нажмите кнопку **запуска** в записной книжке меню и вы увидите «Hello, world!» в выходных данных.

![Image15](media/notebooks-guidance/image15.png)

### <a name="local-python-kernel"></a>Ядро локального python
Выберите локальное ядро Python и в тип ячейки в **

![Image16](media/notebooks-guidance/image16.png)

Вы должны увидеть следующие выходные данные:

![Image17](media/notebooks-guidance/image17.png)

### <a name="markdown-text"></a>Текст с разметкой markdown
Добавьте новую ячейку текст, нажав кнопку + в текстовой команде в панели инструментов.

![Image18](media/notebooks-guidance/image18.png)

Щелкните значок предварительного просмотра, чтобы добавить markdown

![Image19](media/notebooks-guidance/image19.png)

Щелкните значок предварительного просмотра еще раз, чтобы переключиться на просмотр просто markdown

![Image20](media/notebooks-guidance/image20.png)

## <a name="manage-packages"></a>Управление пакетами
Одной из вещей, которые мы оптимизировали для локальной разработки Python было включают возможность установить пакеты, которые клиенты понадобится для своих сценариев. По умолчанию, мы включаем общие пакеты, как pandas, numpy и т.д., но если вы предполагаете, пакет, который не был включен затем написать следующий код в ячейку записной книжки

```python
import <package-name>
```

При выполнении этой команды вы получите `Module not found` ошибки. Если пакет существует, то не будет ошибка.

Если вы нашли `Module not Found` ошибки, а затем щелкните **управление пакетами** для запуска терминала на путь для вашей Virtualenv определены. Теперь можно установить пакеты локально. Чтобы установить пакеты, используйте следующую команду:

```
./pip install <package-name>
```

После установки пакета вы сможете перейдите в ячейку записной книжки и введите следующую команду:

```python
import <package-name>
```

Теперь при выполнении ячейки, вы больше не должны получить `Module not found` ошибки.

Если вы хотите удалить пакет, используйте следующую команду из терминала:

```
./pip uninstall <package-name>
```

## <a name="next-steps"></a>Следующие шаги

Чтобы научиться работать с существующую записную книжку, см. в разделе [управление записных книжек в Azure Data Studio](notebooks-how-to-manage.md).