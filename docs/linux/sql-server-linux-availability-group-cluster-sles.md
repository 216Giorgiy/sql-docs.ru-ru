---
title: "Настройка SLES кластера для группы доступности SQL Server | Документы Microsoft"
description: 
author: MikeRayMSFT
ms.author: mikeray
manager: jhubbard
ms.date: 05/17/2017
ms.topic: article
ms.prod: sql-linux
ms.technology: database-engine
ms.assetid: 85180155-6726-4f42-ba57-200bf1e15f4d
ms.translationtype: MT
ms.sourcegitcommit: 1419847dd47435cef775a2c55c0578ff4406cddc
ms.openlocfilehash: 50a2633790b9878a8be2a9a3c417fc877a37633d
ms.contentlocale: ru-ru
ms.lasthandoff: 08/02/2017

---
# <a name="configure-sles-cluster-for-sql-server-availability-group"></a>Настройка SLES кластера для группы доступности SQL Server

[!INCLUDE[tsql-appliesto-sslinux-only](../../docs/includes/tsql-appliesto-sslinux-only.md)]

Это руководство содержит инструкции для создания трех узлов кластера для SQL Server в SUSE Linux Enterprise Server (SLES) 12 SP2. Для обеспечения высокой доступности группы доступности для Linux требует трех узлов — см. [высокий уровень доступности и защиты данных в конфигурации группы доступности](sql-server-linux-availability-group-ha.md). Кластеризации уровень основан на SUSE [высокий уровень доступности расширения (для которых Имеется)](https://www.suse.com/products/highavailability) построены на основе [Pacemaker](http://clusterlabs.org/). 

Дополнительные сведения о конфигурации кластера, параметры агента ресурсов, управления, советы и рекомендации см. в разделе [SUSE Linux Enterprise высокого уровня доступности расширения 12 SP2](https://www.suse.com/documentation/sle-ha-12/index.html).

>[!NOTE]
>На этом этапе интеграции SQL Server с Pacemaker в Linux не, а также как WSFC в Windows. Службы SQL Server в Linux не является кластерной. Pacemaker элементы управления взаимодействием ресурсов кластера, включая группы доступности. В Linux не следует полагаться на всегда на доступности группы динамические административные представления (DMV), которые предоставляют сведения о кластере, например sys.dm_hadr_cluster. Кроме того имя виртуальной сети доступен только в WSFC, не имеет эквивалента в одной и той же в Pacemaker. Вы можете создать прослушиватель, чтобы использовать его для прозрачного переподключения после отработки отказа, но необходимо будет вручную зарегистрировать имя прослушивателя на DNS-сервере с помощью IP-адрес, используемый для создания виртуального IP-ресурс (как описано ниже).


## <a name="roadmap"></a>Стратегия

Инструкции по созданию группы доступности на серверах Linux для обеспечения высокой доступности, отличаются от действия, указанные на отказоустойчивом кластере Windows Server. Ниже приводятся шаги высокого уровня. 

1. [Настройка SQL Server на узлах кластера](sql-server-linux-setup.md).

2. [Создание группы доступности](sql-server-linux-availability-group-failover-ha.md). 

3. Настройте диспетчер ресурсов кластера, например Pacemaker. Эти инструкции приведены в этом документе.
   
   Способ настройки диспетчер ресурсов кластера зависит от конкретных дистрибутивах Linux. 

   >[!IMPORTANT]
   >Рабочих средах требуется агент разграничения, например STONITH для обеспечения высокой доступности. Демонстрации в этой документации не используйте разграничения агентов. Для тестирования и проверки только являются демонстраций. 
   
   >Кластер Pacemaker использует разграничения для возвращения известного состояния кластера. Способ настройки разграничения зависит от того, распределение и среды. В настоящее время разграничения недоступна в некоторых средах облака. В разделе [SUSE Linux Enterprise высокого уровня доступности расширения](https://www.suse.com/documentation/sle-ha-12/singlehtml/book_sleha/book_sleha.html#cha.ha.fencing).

5. [Добавление группы доступности в качестве ресурса кластера](sql-server-linux-availability-group-cluster-sles.md#configure-the-cluster-resources-for-sql-server). 

## <a name="prerequisites"></a>Предварительные требования

Для выполнения сценария конца в конец ниже необходимо трех компьютеров для развертывания трех узлов кластера. Далее рассматриваются шаги для настройки этих серверов.

## <a name="setup-and-configure-the-operating-system-on-each-cluster-node"></a>Установка и настройка операционной системы на каждом узле кластера 

Первым шагом является настройка операционной системы на узлах кластера. Для этого пошагово используйте SLES 12 SP2 с действительной подпиской для надстройки высокого уровня ДОСТУПНОСТИ.

### <a name="install-and-configure-sql-server-service-on-each-cluster-node"></a>Установка и настройка службы SQL Server на каждом узле кластера

1. Установка и настройка службы SQL Server на всех узлах. Подробные сведения содержатся в разделе [Установка SQL Server в Linux](sql-server-linux-setup.md).

1. Назначить один узел в качестве основного и узлы как баз данных-получателей. Использовать эти термины в этом руководстве.

1. Убедитесь, что узлы, которые будут входить в состав кластера могут взаимодействовать друг с другом.

   В следующем примере показан `/etc/hosts` дополнений для трех узлов, с именем SLES1, SLES2 и SLES3.

   ```
   127.0.0.1   localhost
   10.128.16.33 SLES1
   10.128.16.77 SLES2
   10.128.16.22 SLES3
   ```

   Все узлы кластера должны иметь доступ к друг с другом через SSH. Средства, например `hb_report` или `crm_report` (для устранения неполадок) и Hawk в обозревателе журнала требуется passwordless SSH доступ между узлами, в противном случае их можно будет собирать только данные из текущего узла. В случае, если используется нестандартный порт SSH, используйте параметр -X (в разделе `man` страницы). Например, если SSH-порта 3479, вызвать `crm_report` с:

   ```bash
   sudo crm_report -X "-p 3479" [...]
   ```

   Дополнительные сведения см. в разделе [руководство по администрированию SLES - прочие раздел](http://www.suse.com/documentation/sle-ha-12/singlehtml/book_sleha/book_sleha.html#sec.ha.troubleshooting.misc).


## <a name="create-a-sql-server-login-for-pacemaker"></a>Создайте имя входа SQL Server для Pacemaker

[!INCLUDE [SLES-Create-SQL-Login](../includes/ss-linux-cluster-pacemaker-create-login.md)]

## <a name="configure-an-always-on-availability-group"></a>Настройка группы доступности Always On

На серверах Linux Настройка группы доступности, а затем настройте ресурсы кластера. Настройка группы доступности, в разделе [Настройка группы доступности AlwaysOn для SQL Server в Linux](sql-server-linux-availability-group-configure-ha.md)

## <a name="install-and-configure-pacemaker-on-each-cluster-node"></a>Установка и настройка Pacemaker на каждом узле кластера

1. Установите расширение высокого уровня доступности

   Справочную информацию см. в разделе [установке SUSE Linux Enterprise Server и расширения высокого уровня доступности](https://www.suse.com/documentation/sle-ha-12/singlehtml/install-quick/install-quick.html#sec.ha.inst.quick.installation)

1. Установка пакета агента SQL Server ресурса на обоих узлах.

   ```bash
   sudo zypper install mssql-server-ha
   ```

## <a name="set-up-the-first-node"></a>Установите первый узел

   Ссылаться на [SLES инструкции по установке](http://www.suse.com/documentation/sle-ha-12/singlehtml/install-quick/install-quick.html#sec.ha.inst.quick.setup.1st-node)

1. Войдите в систему как `root` физической или виртуальной машины, которые вы хотите использовать в качестве узла кластера.
2. Запуск сценария начальной загрузки, выполнив:
   ```bash
   sudo ha-cluster-init
   ```

   Если не был настроен NTP для запуска во время загрузки, появится сообщение. 

   Если принято решение продолжить скрипт автоматически создавать ключи для доступа SSH и средство синхронизации Csync2 и запустить службы, необходимые для обоих. 

3. Настройка уровня связи кластеров (Corosync): 

   A. Введите сетевой адрес для привязки. По умолчанию сценарий предложит eth0 сетевой адрес. Также можно ввести отдельный сетевой адрес, например, адрес bond0. 

   Б. Введите адрес многоадресной рассылки. Сценарий предлагает случайно выбранный адрес, который можно использовать в качестве значения по умолчанию. 

   в. Введите порт многоадресной рассылки. Скрипт 5405 предлагает по умолчанию. 

   г. Чтобы настроить `SBD ()`, введите постоянного пути к разделу устройства блок, который вы хотите использовать для одновременной передачи данных. Путь должен быть согласован во всех узлах в кластере. 
   Наконец сценарий запускает службу Pacemaker открыть для одного узла кластера, и затем включите веб-интерфейса управления Hawk2. URL-адрес, используемый для Hawk2 отображается на экране. 

4. Любые сведения о процессе установки проверьте `/var/log/sleha-bootstrap.log`. Теперь у вас есть выполнение одного узла кластера. Проверьте состояние кластера с состоянием crm:

   ```bash
   sudo crm status
   ```

   Можно также просмотреть конфигурации кластера с `crm configure show xml` или `crm configure show`.

5. Процедура начальной загрузки создает пользователя Linux с именем hacluster с linux пароль. Как можно быстрее замените более безопасный пароль по умолчанию: 

   ```bash
   sudo passwd hacluster
   ```

## <a name="add-nodes-to-the-existing-cluster"></a>Добавить узлы к существующему кластеру

При наличии на кластер под управлением с одного или нескольких узлов, добавьте дополнительные узлы кластера с помощью сценария начальной загрузки высокой доступности кластеров соединения. Сценарий требуется только доступ к существующим узлом кластера и автоматически завершать базовая настройка на текущем компьютере. Выполните следующие действия:

Если вы настроили существующие узлы кластера с `YaST` кластера модуль, убедитесь, что будут выполнены следующие предварительные требования перед запуском `ha-cluster-join`:
- Пользователь root на существующих узлах располагает ключей SSH для passwordless входа. 
- `Csync2`настраивается на существующих узлах. Дополнительные сведения см. Настройка Csync2 с YaST. 

1. Войдите в качестве корневой для физической или виртуальной машины, предназначен для присоединения к кластеру. 
2. Запуск сценария начальной загрузки, выполнив: 

   ```bash
   sudo ha-cluster-join
   ```

   Если не был настроен NTP для запуска во время загрузки, появится сообщение. 

3. В случае продолжения будет приглашение для IP-адреса существующего узла. Введите IP-адрес. 

4. Если вы еще не настроили passwordless SSH доступа между обеих машинах, вы также предложит ввести пароль учетной записи root существующего узла. 

   После входа в систему для указанного узла, сценарий будет скопировать конфигурацию Corosync, настройте SSH и `Csync2`и появится на текущем компьютере сети как новый узел кластера. Кроме того, она запускает службу, необходимые для Hawk. Если вы настроили общее хранилище с `OCFS2`, автоматически создаст каталог монтирования для `OCFS2` файловой системы. 

5. Повторите предыдущие шаги для всех компьютеров, которые требуется добавить в кластер. 

6. Сведения о процессе проверки `/var/log/ha-cluster-bootstrap.log`. 

1. Проверьте состояние кластера с `sudo crm status`. Если второй узел добавлен успешно, выходные данные должны быть примерно следующими:

   ```bash
   sudo crm status
   
   3 nodes configured
   1 resource configured
   Online: [ SLES1 SLES2 SLES3]
   Full list of resources:   
   admin_addr     (ocf::heartbeat:IPaddr2):       Started node1
   ```

   >[!NOTE]
   >`admin_addr`представляет виртуальные IP-адреса кластера, настроенный во время установки исходного одного узла кластера.

После добавления всех узлов, проверьте, если вам потребуется изменить нет кворума policy в параметрах глобального кластера. Это особенно важно для кластеров с двумя узлами. Дополнительные сведения см в разделе 4.1.2, параметр нет кворума policy. 

## <a name="set-cluster-property-start-failure-is-fatal-to-false"></a>Значение свойства кластера start сбой является Неустранимая false

`Start-failure-is-fatal`Указывает ли сбой запуска ресурса на узле предотвращает последующие попытки запуска на этом узле. Если задано значение `false`, кластер решить, следует ли попробуйте запустить на том же узле, еще раз на основании ресурса текущего счетчика и миграции Порог сбоя. Таким образом после перехода на другой ресурс, Pacemaker повторит попытку начиная ресурс группы доступности на первый основной после экземпляра SQL Server. Pacemaker берет на себя при понижении роли реплики во вторичные и он автоматически будет повторно подключиться к группе доступности. Кроме того Если `start-failure-is-fatal` равно `false`, кластере вернется к настроенным failcount ограничения, настроенного с пороговым значением миграции, поэтому необходимо убедиться, что по умолчанию для порога миграции обновляется соответствующим образом.

Чтобы обновить значение свойства false выполнять:
```bash
sudo crm configure property start-failure-is-fatal=false
sudo crm configure rsc_defaults migration-threshold=5000
```
Если свойство имеет значение по умолчанию `true`, если первая попытка запустить ресурс, вмешательства пользователя после автоматического перехода на количество сбоев ресурсов очистки и сброса конфигурации с помощью: `sudo crm resource cleanup <resourceName>` команды.

Дополнительные сведения о Pacemaker свойства кластера см. в разделе [Настройка ресурсов кластера](https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_config_crm_resources.html).

# <a name="configure-fencing-stonith"></a>Настройка разграничения (STONITH)
Pacemaker кластера поставщиков требуют STONITH должно быть включено и разграничения устройству, настроенному для поддерживаемых кластера. Если диспетчер ресурсов кластера не удается определить состояние узла или ресурса на узле, разграничения позволяет снова подключить известного состояния кластера.
Ресурс уровня разграничения главным образом, гарантируется без повреждения данных в случае сбоя путем настройки ресурса. Можно использовать разграничения уровня ресурсов, например, с DRBD (распределенных реплицируются блоков) для пометки диска на узле как устаревший, когда канал связи выходит из строя.
Узел уровня разграничения гарантирует узла не выполняет какие-либо ресурсы. Это делается путем сброса узел и его реализация Pacemaker вызывается STONITH (что означает «прокрутить другой узел в начало»). Pacemaker поддерживает выполнения разнообразных разграничения устройства, например бесперебойного питания или управления интерфейсные карты для серверов.
Дополнительные сведения см. в разделе [Pacemaker кластеры с нуля](http://clusterlabs.org/doc/en-US/Pacemaker/1.1-plugin/html/Clusters_from_Scratch/ch05.html), [разграничения и Stonith](http://clusterlabs.org/doc/crm_fencing.html) и [SUSE высокой ДОСТУПНОСТИ документации: разграничения и STONITH](https://www.suse.com/documentation/sle_ha/book_sleha/data/cha_ha_fencing.html).

Во время инициализации кластера STONITH будет отключен, если конфигурация не обнаруживается. Его можно включить позже, запустив следующий командой

```bash
sudo crm configure property stonith-enabled=true
```
  
>[!IMPORTANT]
>Отключение STONITH — только для целей тестирования. Если вы планируете использовать Pacemaker в рабочей среде, следует планировать реализацию STONITH в зависимости от среды и хранить включена. Обратите внимание, что SUSE не предоставляет разграничения агентов для облачных сред (включая Azure) или Hyper-V. Следовательно поставщика кластера не обеспечивает поддержку для запуска рабочих кластерах в этих средах. Мы работаем над решением для пропуска, будут доступны в будущих выпусках.


## <a name="configure-the-cluster-resources-for-sql-server"></a>Настройка ресурсов кластера для SQL Server

Ссылаться на [SLES администрирования Guid](https://www.suse.com/documentation/sle-ha-12/singlehtml/book_sleha/book_sleha.html#cha.ha.manual_config)

### <a name="create-availability-group-resource"></a>Создание группы доступности

Следующая команда создает и настраивает группы доступности для 3 реплик группы доступности [ag1]. Монитор операций и увеличение времени ожидания должны быть явно указаны в SLES исходя из того, что время ожидания, высокой рабочей нагрузки зависимых и тщательно настраивается для каждого развертывания.
На одном из узлов в кластере, выполните команду:

1. Запустите `crm configure` Открытие crm строки:

   ```bash
   sudo crm configure 
   ```

1. В строке crm выполните следующую команду для настройки свойств ресурса.

   ```bash
primitive ag_cluster \
   ocf:mssql:ag \
   params ag_name="ag1" \
   op start timeout=60s \
   op stop timeout=60s \
   op promote timeout=60s \
   op demote timeout=10s \
   op monitor timeout=60s interval=10s \
   op monitor timeout=60s interval=11s role="Master" \
   op monitor timeout=60s interval=12s role="Slave" \
   op notify timeout=60s
ms ms-ag_cluster ag_cluster \
   meta master-max="1" master-node-max="1" clone-max="3" \
  clone-node-max="1" notify="true" \
commit
   ```

[!INCLUDE [required-synchronized-secondaries-default](../includes/ss-linux-cluster-required-synchronized-secondaries-default.md)]

### <a name="create-virtual-ip-resource"></a>Создайте виртуальные IP-адреса

Если вы не создали виртуальные IP-адреса при запуске `ha-cluster-init` теперь можно создать этот ресурс. Следующая команда создает виртуальные IP-адреса. Замените `<**0.0.0.0**>` с доступный адрес из локальной сети и `<**24**>` с количеством биты в маске подсети CIDR. Запустите на одном узле.

```bash
crm configure \
primitive admin_addr \
   ocf:heartbeat:IPaddr2 \
   params ip=<**0.0.0.0**> \
      cidr_netmask=<**24**>
```

### <a name="add-colocation-constraint"></a>Добавить ограничение совместного размещения
Практически каждое решение в кластере Pacemaker, такие как выбор, где должны запускаться ресурс выполняется путем сравнения оценок. Вычисления оценки для каждого ресурса, и диспетчер ресурсов кластера выбирает узел с высший показатель для конкретного ресурса. (Если узел имеет отрицательный показатель для ресурса, ресурс не может выполняться на этом узле.) Манипулирования решения кластера с ограничениями. Ограничения имеют оценку. Если ограничение имеет показатель меньше БЕСКОНЕЧНОСТИ, это только рекомендация. Оценка бесконечность означает, что он является обязательным. Мы хотим убедитесь, что первичной группы доступности и виртуальной IP-ресурс запущена на том же узле, поэтому мы определим ограничение совместного размещения с оценкой бесконечность. 

Чтобы задать ограничение совместного размещения для виртуального IP-адреса для запуска на одном узле роль хозяина, выполните следующую команду на одном узле:

```bash
crm configure
colocation vip_on_master inf: \
    admin_addr ms-ag_cluster:Master
commit
```

### <a name="add-ordering-constraint"></a>Добавить ограничение порядка сортировки
Ограничение совместного размещения имеет неявное ограничение порядка сортировки. Он перемещает виртуальные IP-адреса, прежде чем перейти в ресурс группы доступности. По умолчанию является последовательность событий: 

1. Ресурс пользователя проблемы миграции образец группы доступности с node1 на узле2.
2. Виртуальные IP-адреса останавливается на узле 1.
3. Виртуальные IP-адреса запускается на узле 2. На этом этапе IP-адрес временно указывает на узел 2 пока узел 2 по-прежнему pre-отработки отказа Вторичная. 
4. Образец группы доступности на узле 1 будет понижен до подчиненных.
5. Ведомый группы доступности на узле 2 повышается до главного. 

Чтобы предотвратить временно указывающие на узел с данными дополнительных pre-отработки отказа IP-адрес, добавьте ограничением порядка сортировки. Чтобы добавить упорядочивания ограничение, выполните следующую команду на одном узле: 

```bash
crm crm configure \
   order ag_first inf: ms-ag_cluster:promote admin_addr:start
```


>[!IMPORTANT]
>После настройки кластера и добавить группу доступности в качестве ресурса кластера, Transact-SQL нельзя использовать для отработки отказа ресурсов группы доступности. Ресурсы кластера SQL Server в Linux не связаны тесно как с операционной системой, как и на кластере отработки отказа Windows Server (WSFC). Службы SQL Server не имеет сведений о присутствии кластера. Все взаимодействие осуществляется с помощью средства управления кластером. В SLES использовать `crm`. 

Вручную переключить группу доступности с `crm`. Инициирует переход на другой ресурс с помощью Transact-SQL. Инструкции см. в разделе [перехода на другой ресурс](sql-server-linux-availability-group-failover-ha.md#failover).


Дополнительные сведения см: в разделе
- [Управление ресурсами кластера](https://www.suse.com/documentation/sle-ha-12/singlehtml/book_sleha/book_sleha.html#sec.ha.config.crm).   
- [HA основные понятия](https://www.suse.com/documentation/sle-ha-12/singlehtml/book_sleha/book_sleha.html#cha.ha.concepts)
- [Краткий справочник pacemaker](https://github.com/ClusterLabs/pacemaker/blob/master/doc/pcs-crmsh-quick-ref.md) 

<!---[!INCLUDE [Pacemaker Concepts](..\includes\ss-linux-cluster-pacemaker-concepts.md)]--->

## <a name="next-steps"></a>Следующие шаги

[Работать с высокой ДОСТУПНОСТИ группы доступности](sql-server-linux-availability-group-failover-ha.md)
